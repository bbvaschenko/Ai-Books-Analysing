"""
–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF –∏ —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
"""
import re
import os
import torch
import torch.nn.functional as F
import pandas as pd
import fitz  # PyMuPDF
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º AI –∞–≥–µ–Ω—Ç–∞
from ai_agent import EducationalAIAgent, BookAnalysis as AIBookAnalysis

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—á–µ–±–Ω–æ—Å—Ç–∏
from autonomous_classifier import AutonomousEducationalClassifier, quick_educational_check


@dataclass
class BookData:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –∫–Ω–∏–≥–µ"""
    book_number: int
    book_id: str
    filename: str
    area: str
    tags: Dict[str, List[str]]
    text: Optional[str] = None
    embedding: Optional[torch.Tensor] = None
    ai_analysis: Optional[AIBookAnalysis] = None  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞


class PDFProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF —Ñ–∞–π–ª–æ–≤"""

    @staticmethod
    def clean_text(text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤"""
        return re.sub(r'\s+', ' ', text.strip())

    @staticmethod
    def extract_text(pdf_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–∞"""
        try:
            doc = fitz.open(pdf_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return PDFProcessor.clean_text(text)
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF: {str(e)}")

    @staticmethod
    def validate_text(text: str, min_length: int = 200) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        return text and len(text.strip()) >= min_length


class EmbeddingModel:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""

    def __init__(self, model_name: str = "Qwen/Qwen3-Embedding-0.6B"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()

        # –î–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–µ–≥–∞–º
        self.search_model = SentenceTransformer(
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )

    def get_text_embedding(self, text: str, chunk_size: int = 512) -> torch.Tensor:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        embeddings = []

        for chunk in chunks:
            inputs = self.tokenizer(
                chunk,
                return_tensors='pt',
                truncation=True,
                padding=True,
                max_length=512
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)

            emb = outputs.last_hidden_state.mean(dim=1)
            embeddings.append(emb.cpu())

        return torch.mean(torch.stack(embeddings), dim=0)

    def get_tag_embeddings(self, tags: List[str]) -> torch.Tensor:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–≥–æ–≤"""
        tag_embeddings = []

        for tag in tags:
            inputs = self.tokenizer(
                tag,
                return_tensors='pt',
                truncation=True,
                padding=True,
                max_length=10
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)

            tag_embedding = outputs.last_hidden_state.mean(dim=1)
            tag_embeddings.append(tag_embedding.cpu())

        return torch.cat(tag_embeddings)

    def find_similar_tags(self, query: str, all_tags: List[str], top_k: int = 8) -> List[str]:
        """–ü–æ–∏—Å–∫ —Ç–µ–≥–æ–≤, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ –∑–∞–ø—Ä–æ—Å"""
        query = query.lower()
        query = re.sub(r'[^\w\s–∞-—è—ë-]', ' ', query)
        query = re.sub(r'\s+', ' ', query).strip()

        if len(query) < 3:
            return []

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.search_model.encode(
            query,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö —Ç–µ–≥–æ–≤
        tag_embeddings = []
        for tag in all_tags:
            tag_emb = self.search_model.encode(
                tag,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            tag_embeddings.append(tag_emb)

        tag_embeddings = np.array(tag_embeddings)

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        similarities = cosine_similarity(
            query_embedding.reshape(1, -1),
            tag_embeddings
        )[0]

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
        sorted_indices = np.argsort(similarities)[::-1][:top_k]
        return [all_tags[i] for i in sorted_indices if similarities[i] > 0.3]


class TagManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–≥–∞–º–∏"""

    def __init__(self, tags_directory: str = "tags"):
        self.tags_directory = tags_directory
        self.tags_dict = self.load_tags()
        self.section_to_area = self.load_area_mapping()

    def load_tags(self) -> Dict[str, List[str]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        tags_dict = {}

        if not os.path.exists(self.tags_directory):
            os.makedirs(self.tags_directory)
            print(f"–°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {self.tags_directory}")
            return tags_dict

        for filename in os.listdir(self.tags_directory):
            if filename.endswith(".txt") and filename != "–æ–±–ª–∞—Å—Ç–∏_–∑–Ω–∞–Ω–∏–π.txt":
                category = filename[:-4]  # —É–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ .txt
                filepath = os.path.join(self.tags_directory, filename)

                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        tags = [line.strip() for line in f if line.strip()]
                        tags_dict[category] = tags
                        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(tags)} —Ç–µ–≥–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}'")
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–≥–æ–≤ –∏–∑ {filename}: {e}")
                    tags_dict[category] = []

        return tags_dict

    def load_area_mapping(self, mapping_file: str = "tags/–æ–±–ª–∞—Å—Ç–∏_–∑–Ω–∞–Ω–∏–π.txt") -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π —Ä–∞–∑–¥–µ–ª–æ–≤ –∏ –æ–±–ª–∞—Å—Ç–µ–π –∑–Ω–∞–Ω–∏–π"""
        section_to_area = {}

        if os.path.exists(mapping_file):
            try:
                with open(mapping_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and ':' in line:
                            section, area = line.split(':', 1)
                            section_to_area[section.strip()] = area.strip()
                print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(section_to_area)} —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π —Ä–∞–∑–¥–µ–ª–æ–≤ –∏ –æ–±–ª–∞—Å—Ç–µ–π")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π: {e}")
        else:
            print(f"–§–∞–π–ª —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è–º–∏ {mapping_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")

        return section_to_area

    def get_all_tags_flat(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–µ–≥–æ–≤ –≤ –æ–¥–Ω–æ–º —Å–ø–∏—Å–∫–µ"""
        all_tags = []
        for tags_list in self.tags_dict.values():
            all_tags.extend(tags_list)
        return all_tags

    def infer_area_from_sections(self, section_tags_found: List[str]) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–∏ –∑–Ω–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤"""
        area_counter = {}
        for section in section_tags_found:
            area = self.section_to_area.get(section)
            if area:
                area_counter[area] = area_counter.get(area, 0) + 1

        if area_counter:
            sorted_areas = sorted(area_counter.items(), key=lambda x: -x[1])
            return [sorted_areas[0][0]]
        return ["–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"]


class BookAnalyzer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–Ω–∏–≥"""

    def __init__(self, excel_file: str = "analyzed_books.xlsx"):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É Excel
        self.excel_file = os.path.abspath(excel_file)
        print(f"üìÅ –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {self.excel_file}")

        self.tag_manager = TagManager()
        self.embedding_model = EmbeddingModel()
        self.pdf_processor = PDFProcessor()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —É—á–µ–±–Ω–æ—Å—Ç–∏
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã...")
        self.educational_classifier = AutonomousEducationalClassifier()
        print("‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—á–µ–±–Ω–æ—Å—Ç–∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –∞–≥–µ–Ω—Ç–∞
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        self.ai_agent = EducationalAIAgent()
        print("‚úÖ AI –∞–≥–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —Å–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª Excel –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        self._ensure_excel_file()

    def _ensure_excel_file(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª Excel —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç"""
        if not os.path.exists(self.excel_file):
            print(f"üìÑ –°–æ–∑–¥–∞—é –Ω–æ–≤—ã–π —Ñ–∞–π–ª Excel: {self.excel_file}")

            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É DataFrame
            df = pd.DataFrame(columns=[
                "–ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏", "ID –∫–Ω–∏–≥–∏", "–ò–º—è —Ñ–∞–π–ª–∞", "–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π", "–¢–µ–∫—Å—Ç (—Ñ—Ä–∞–≥–º–µ–Ω—Ç)",
                "–†–∞–∑–¥–µ–ª—ã", "–ü—Ä–µ–¥–º–µ—Ç—ã", "–ö–ª–∞—Å—Å—ã", "–ê–≤—Ç–æ—Ä—ã", "–¢–µ–º—ã",
                "AI_–†–µ–∑—é–º–µ", "AI_–†–∞–∑–¥–µ–ª—ã", "AI_–ö–ª—é—á–µ–≤—ã–µ_—Ç–µ–º—ã", "AI_–£—Ä–æ–≤–µ–Ω—å_—Å–ª–æ–∂–Ω–æ—Å—Ç–∏",
                "AI_–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ_–∑–Ω–∞–Ω–∏—è", "AI_–û–±–ª–∞—Å—Ç–∏_–º–∞—Ç–µ–º–∞—Ç–∏–∫–∏", "AI_–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
            ])

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
            df.to_excel(self.excel_file, index=False)
            print(f"‚úÖ –§–∞–π–ª Excel —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        else:
            print(f"‚úÖ –§–∞–π–ª Excel —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {self.excel_file}")

    def get_next_book_number(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –Ω–æ–º–µ—Ä–∞ –∫–Ω–∏–≥–∏"""
        if not os.path.exists(self.excel_file):
            return 1

        try:
            df = pd.read_excel(self.excel_file)
            if df.empty or '–ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏' not in df.columns:
                return 1

            # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä –∫–Ω–∏–≥–∏
            max_number = df['–ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏'].max()
            if pd.isna(max_number):
                return 1
            return int(max_number) + 1
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –Ω–æ–º–µ—Ä–∞ –∫–Ω–∏–≥–∏: {e}")
            return 1

    def find_tags_for_text(self, text_embedding: torch.Tensor,
                          tag_list: List[str], top_k: int = 3) -> List[str]:
        """–ü–æ–∏—Å–∫ —Ç–µ–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if not tag_list:
            return []

        try:
            tag_embeddings = self.embedding_model.get_tag_embeddings(tag_list)
            similarities = F.cosine_similarity(text_embedding, tag_embeddings)
            top_indices = similarities.topk(min(top_k, len(tag_list))).indices
            return [tag_list[i] for i in top_indices]
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ç–µ–≥–æ–≤: {e}")
            return []

    def analyze_book(self, pdf_path: str) -> Optional[BookData]:
        """–ê–Ω–∞–ª–∏–∑ –∫–Ω–∏–≥–∏ –∏–∑ PDF —Ñ–∞–π–ª–∞"""
        print(f"\n{'='*60}")
        print(f"–ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {pdf_path}")
        print(f"{'='*60}")

        if not os.path.exists(pdf_path):
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")
            return None

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        print("üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF...")
        try:
            raw_text = self.pdf_processor.extract_text(pdf_path)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF: {e}")
            return None

        if not self.pdf_processor.validate_text(raw_text):
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
            return None

        # üîç –ê–í–¢–û–ù–û–ú–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ù–ê –£–ß–ï–ë–ù–£–Æ –õ–ò–¢–ï–†–ê–¢–£–†–£
        print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—á–µ–±–Ω—É—é –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—É...")
        check_result = self.educational_classifier.check_if_educational(raw_text)

        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {'‚úÖ –£–ß–ï–ë–ù–ê–Ø' if check_result['is_educational'] else '‚ùå –ù–ï —É—á–µ–±–Ω–∞—è'}")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {check_result['confidence']:.0%}")
        print(f"   –ü—Ä–∏—á–∏–Ω–∞: {check_result['reason']}")

        if check_result['criteria_met']['has_mathematics']:
            print(f"   üî¢ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã")

        if not check_result['is_educational']:
            print(f"\n‚ùå –§–∞–π–ª –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π!")
            print(f"   –§–∞–π–ª –æ—Ç–∫–ª–æ–Ω–µ–Ω.")
            return None

        print(f"\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        print("\nüß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç—É...")
        try:
            text_embedding = self.embedding_model.get_text_embedding(raw_text)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            return None

        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        print("\nüè∑Ô∏è  –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤...")
        found_tags = {}

        for category, tags in self.tag_manager.tags_dict.items():
            if category == "–æ–±–ª–∞—Å—Ç–∏_–∑–Ω–∞–Ω–∏–π":
                continue

            found_tags[category] = self.find_tags_for_text(text_embedding, tags, top_k=3)
            if found_tags[category]:
                print(f"   {category}: {', '.join(found_tags[category])}")

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–∏ –∑–Ω–∞–Ω–∏–π
        found_area = self.tag_manager.infer_area_from_sections(
            found_tags.get("—Ä–∞–∑–¥–µ–ª—ã", [])
        )
        print(f"   üß≠ –û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π: {found_area[0]}")

        # AI –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∫–Ω–∏–≥–∏
        print("\nü§ñ –ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ AI –∞–Ω–∞–ª–∏–∑–∞...")
        ai_analysis = None
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            analysis_text = raw_text[:10000]  # –ü–µ—Ä–≤—ã–µ 10000 —Å–∏–º–≤–æ–ª–æ–≤
            ai_analysis = self.ai_agent.analyze_book_content(analysis_text, found_tags)
            print("‚úÖ AI –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")

            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã AI –∞–Ω–∞–ª–∏–∑–∞
            print("\n" + "="*60)
            print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ AI –ê–ù–ê–õ–ò–ó–ê")
            print("="*60)
            print(f"üìù –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ: {ai_analysis.summary[:200]}...")
            print(f"üìà –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {ai_analysis.difficulty_level}")
            if ai_analysis.mathematical_areas:
                print(f"üî¢ –û–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏: {', '.join(ai_analysis.mathematical_areas)}")
            if ai_analysis.key_topics:
                print(f"üè∑Ô∏è  –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã: {', '.join(ai_analysis.key_topics[:5])}")
            if ai_analysis.recommendations:
                print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {ai_analysis.recommendations[0]}")
            print("="*60)

        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ AI –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
            print("–ü—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")

        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥–∏
        book_number = self.get_next_book_number()
        book_id = f"{book_number:04d}"

        print(f"\nüìö –°–æ–∑–¥–∞—é –∑–∞–ø–∏—Å—å –æ –∫–Ω–∏–≥–µ:")
        print(f"   –ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏: {book_number}")
        print(f"   ID –∫–Ω–∏–≥–∏: {book_id}")
        print(f"   –ò–º—è —Ñ–∞–π–ª–∞: {os.path.basename(pdf_path)}")

        book_data = BookData(
            book_number=book_number,
            book_id=book_id,
            filename=os.path.basename(pdf_path),
            area=', '.join(found_area),
            tags=found_tags,
            text=raw_text[:500] + "..." if len(raw_text) > 500 else raw_text,
            embedding=text_embedding,
            ai_analysis=ai_analysis
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ Excel
        try:
            self.save_to_database(book_data)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ Excel: {e}")
            return None

        return book_data

    def save_to_database(self, book_data: BookData):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥–∏ –≤ Excel"""
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Excel...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        book_dict = {
            "–ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏": book_data.book_number,
            "ID –∫–Ω–∏–≥–∏": book_data.book_id,
            "–ò–º—è —Ñ–∞–π–ª–∞": book_data.filename,
            "–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π": book_data.area,
            "–¢–µ–∫—Å—Ç (—Ñ—Ä–∞–≥–º–µ–Ω—Ç)": book_data.text
        }

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category, tags in book_data.tags.items():
            column_name = category.capitalize()
            book_dict[column_name] = ', '.join(tags) if tags else ""

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ AI –∞–Ω–∞–ª–∏–∑–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if book_data.ai_analysis:
            ai_data = book_data.ai_analysis

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º BookAnalysis –≤ —Å–ª–æ–≤–∞—Ä—å
            ai_dict = asdict(ai_data)

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            book_dict["AI_–†–µ–∑—é–º–µ"] = ai_dict['summary'][:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

            if ai_dict['sections_summary']:
                sections_str = ' | '.join(
                    f"{k}: {v[:100]}"
                    for k, v in list(ai_dict['sections_summary'].items())[:3]
                )
                book_dict["AI_–†–∞–∑–¥–µ–ª—ã"] = sections_str[:300]
            else:
                book_dict["AI_–†–∞–∑–¥–µ–ª—ã"] = ""

            book_dict["AI_–ö–ª—é—á–µ–≤—ã–µ_—Ç–µ–º—ã"] = ', '.join(ai_dict['key_topics'][:5]) if ai_dict['key_topics'] else ""
            book_dict["AI_–£—Ä–æ–≤–µ–Ω—å_—Å–ª–æ–∂–Ω–æ—Å—Ç–∏"] = ai_dict['difficulty_level']
            book_dict["AI_–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ_–∑–Ω–∞–Ω–∏—è"] = ', '.join(ai_dict['prerequisites'][:3]) if ai_dict['prerequisites'] else ""
            book_dict["AI_–û–±–ª–∞—Å—Ç–∏_–º–∞—Ç–µ–º–∞—Ç–∏–∫–∏"] = ', '.join(ai_dict['mathematical_areas']) if ai_dict['mathematical_areas'] else ""
            book_dict["AI_–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"] = ' | '.join(ai_dict['recommendations'][:3]) if ai_dict['recommendations'] else ""

        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å—å—é
        new_row_df = pd.DataFrame([book_dict])

        try:
            if os.path.exists(self.excel_file):
                # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª
                existing_df = pd.read_excel(self.excel_file)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ –∫–Ω–∏–≥–∏ —Å —Ç–∞–∫–∏–º ID
                if 'ID –∫–Ω–∏–≥–∏' in existing_df.columns:
                    if book_data.book_id in existing_df['ID –∫–Ω–∏–≥–∏'].values:
                        print(f"‚ö†Ô∏è  –ö–Ω–∏–≥–∞ —Å ID {book_data.book_id} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –±–∞–∑–µ")
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
                        mask = existing_df['ID –∫–Ω–∏–≥–∏'] == book_data.book_id
                        existing_df.loc[mask, list(book_dict.keys())] = pd.Series(book_dict)
                        df = existing_df
                    else:
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                        df = pd.concat([existing_df, new_row_df], ignore_index=True)
                else:
                    df = pd.concat([existing_df, new_row_df], ignore_index=True)
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª
                df = new_row_df

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
            df.to_excel(self.excel_file, index=False)
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {self.excel_file}")
            print(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ: {len(df)}")

            # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
            if book_data.ai_analysis:
                self._save_ai_report(book_data)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ Excel: {e}")
            raise

    def _save_ai_report(self, book_data: BookData):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ AI –∞–Ω–∞–ª–∏–∑–∞"""
        report_dir = "ai_reports"
        if not os.path.exists(report_dir):
            os.makedirs(report_dir)

        report_file = os.path.join(report_dir, f"{book_data.book_id}_report.txt")

        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(f"=== –û–¢–ß–ï–¢ AI –ê–ù–ê–õ–ò–ó–ê –ö–ù–ò–ì–ò ===\n\n")
                f.write(f"ID –∫–Ω–∏–≥–∏: {book_data.book_id}\n")
                f.write(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {book_data.filename}\n")
                f.write(f"–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π: {book_data.area}\n\n")

                if book_data.ai_analysis:
                    ai = book_data.ai_analysis

                    f.write("–ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï:\n")
                    f.write(f"{ai.summary}\n\n")

                    f.write("–£–†–û–í–ï–ù–¨ –°–õ–û–ñ–ù–û–°–¢–ò:\n")
                    f.write(f"{ai.difficulty_level}\n\n")

                    if ai.mathematical_areas:
                        f.write("–û–ë–õ–ê–°–¢–ò –ú–ê–¢–ï–ú–ê–¢–ò–ö–ò:\n")
                        for area in ai.mathematical_areas:
                            f.write(f"- {area}\n")
                        f.write("\n")

                    if ai.key_topics:
                        f.write("–ö–õ–Æ–ß–ï–í–´–ï –¢–ï–ú–´:\n")
                        for topic in ai.key_topics[:10]:
                            f.write(f"- {topic}\n")
                        f.write("\n")

                    if ai.prerequisites:
                        f.write("–ù–ï–û–ë–•–û–î–ò–ú–´–ï –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ï –ó–ù–ê–ù–ò–Ø:\n")
                        for prereq in ai.prerequisites:
                            f.write(f"- {prereq}\n")
                        f.write("\n")

                    if ai.sections_summary:
                        f.write("–ê–ù–ê–õ–ò–ó –†–ê–ó–î–ï–õ–û–í:\n")
                        for section, desc in list(ai.sections_summary.items())[:5]:
                            f.write(f"{section}:\n")
                            f.write(f"{desc[:200]}...\n\n")

                    if ai.recommendations:
                        f.write("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–ó–£–ß–ï–ù–ò–Æ:\n")
                        for rec in ai.recommendations:
                            f.write(f"- {rec}\n")

            print(f"üìÑ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")

    def search_books(self, query: str, top_k: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –∫–Ω–∏–≥ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not os.path.exists(self.excel_file):
            return []

        try:
            df = pd.read_excel(self.excel_file)

            # –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–≥–∞–º
            all_tags = self.tag_manager.get_all_tags_flat()
            similar_tags = self.embedding_model.find_similar_tags(query, all_tags)

            if not similar_tags:
                return []

            # –ò—â–µ–º –∫–Ω–∏–≥–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–≥–∏
            results = []
            for _, row in df.iterrows():
                score = 0
                book_tags = []

                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–≥–∏ –∫–Ω–∏–≥–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                for col in df.columns:
                    if col not in ["–ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏", "ID –∫–Ω–∏–≥–∏", "–ò–º—è —Ñ–∞–π–ª–∞",
                                 "–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π", "–¢–µ–∫—Å—Ç (—Ñ—Ä–∞–≥–º–µ–Ω—Ç)"] and not col.startswith("AI_"):
                        if pd.notna(row[col]):
                            book_tags.extend(str(row[col]).split(', '))

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                for tag in similar_tags:
                    if tag in book_tags:
                        score += 1

                if score > 0:
                    result = {
                        'score': score,
                        'book_id': row['ID –∫–Ω–∏–≥–∏'],
                        'filename': row['–ò–º—è —Ñ–∞–π–ª–∞'],
                        'area': row['–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π'],
                        'matching_tags': [t for t in similar_tags if t in book_tags]
                    }

                    # –î–æ–±–∞–≤–ª—è–µ–º AI –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                    if 'AI_–†–µ–∑—é–º–µ' in df.columns and pd.notna(row['AI_–†–µ–∑—é–º–µ']):
                        result['ai_summary'] = str(row['AI_–†–µ–∑—é–º–µ'])[:150] + "..."

                    if 'AI_–£—Ä–æ–≤–µ–Ω—å_—Å–ª–æ–∂–Ω–æ—Å—Ç–∏' in df.columns and pd.notna(row['AI_–£—Ä–æ–≤–µ–Ω—å_—Å–ª–æ–∂–Ω–æ—Å—Ç–∏']):
                        result['ai_difficulty'] = str(row['AI_–£—Ä–æ–≤–µ–Ω—å_—Å–ª–æ–∂–Ω–æ—Å—Ç–∏'])

                    results.append(result)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            results.sort(key=lambda x: x['score'], reverse=True)
            return results[:top_k]

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {str(e)}")
            return []

    def get_book_details(self, book_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ"""
        if not os.path.exists(self.excel_file):
            return None

        try:
            df = pd.read_excel(self.excel_file)
            book_row = df[df['ID –∫–Ω–∏–≥–∏'] == book_id]

            if book_row.empty:
                return None

            row = book_row.iloc[0]

            # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            details = {
                'book_id': row['ID –∫–Ω–∏–≥–∏'],
                'filename': row['–ò–º—è —Ñ–∞–π–ª–∞'],
                'area': row['–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π'],
                'tags': {}
            }

            # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–≥–∏
            for col in df.columns:
                if col not in ["–ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏", "ID –∫–Ω–∏–≥–∏", "–ò–º—è —Ñ–∞–π–ª–∞",
                             "–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π", "–¢–µ–∫—Å—Ç (—Ñ—Ä–∞–≥–º–µ–Ω—Ç)"] and not col.startswith("AI_"):
                    if pd.notna(row[col]):
                        details['tags'][col] = str(row[col]).split(', ')

            # –î–æ–±–∞–≤–ª—è–µ–º AI –¥–∞–Ω–Ω—ã–µ
            ai_fields = [col for col in df.columns if col.startswith("AI_")]
            if ai_fields:
                details['ai_analysis'] = {}
                for field in ai_fields:
                    if pd.notna(row[field]):
                        details['ai_analysis'][field.replace("AI_", "")] = str(row[field])

            return details

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–µ—Ç–∞–ª–µ–π –∫–Ω–∏–≥–∏: {str(e)}")
            return None