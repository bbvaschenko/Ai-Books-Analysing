"""
–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF –∏ —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
"""
import re
import os
import torch
import torch.nn.functional as F
import pandas as pd
import fitz  # PyMuPDF
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º AI –∞–≥–µ–Ω—Ç–∞
from ai_agent import EducationalAIAgent, BookAnalysis as AIBookAnalysis


@dataclass
class BookData:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –∫–Ω–∏–≥–µ"""
    book_number: int
    book_id: str
    filename: str
    area: str
    tags: Dict[str, List[str]]
    text: Optional[str] = None
    embedding: Optional[torch.Tensor] = None
    ai_analysis: Optional[AIBookAnalysis] = None  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞


class AutonomousEducationalClassifier:
    """–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã"""

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def _extract_text_features(self, text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        features = {
            'text_length': len(text),
            'paragraph_count': len(re.split(r'\n\s*\n', text)),
            'sentence_count': len(re.split(r'[.!?]+', text)),
            'avg_sentence_length': 0,
            'vocabulary_richness': 0
        }

        if features['sentence_count'] > 0:
            features['avg_sentence_length'] = len(text) / features['sentence_count']

        # –ê–Ω–∞–ª–∏–∑ –±–æ–≥–∞—Ç—Å—Ç–≤–∞ —Å–ª–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–∞–ø–∞—Å–∞
        words = re.findall(r'\b[–∞-—è–ê-–Ø—ë–Å]{3,}\b', text.lower())
        if words:
            unique_words = set(words)
            features['vocabulary_richness'] = len(unique_words) / len(words) if len(words) > 0 else 0

        return features

    def _analyze_text_structure(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–µ–∫—Å—Ç–∞"""
        structure = {
            'has_numerical_sections': False,
            'has_definitions': False,
            'has_examples': False,
            'has_exercises': False,
            'has_references': False,
            'has_tables_figures': False,
            'section_hierarchy_depth': 0
        }

        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        headings = re.findall(r'(?:–ì–ª–∞–≤–∞|–†–∞–∑–¥–µ–ª|¬ß|–¢–µ–º–∞|–ü–∞—Ä–∞–≥—Ä–∞—Ñ|–ß–∞—Å—Ç—å)\s+[^\n]+', text)
        if headings:
            structure['has_numerical_sections'] = True
            structure['section_hierarchy_depth'] = min(3, len(headings) // 2)

        # –ü–æ–∏—Å–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
        definition_patterns = [
            r'–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\s*[0-9]*[:.]?\s*[^\n]+',
            r'\b–æ–ø—Ä–µ–¥–µ–ª–∏–º\b.*?–∫–∞–∫\b',
            r'\b–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è\b.*?\b–µ—Å–ª–∏\b'
        ]
        for pattern in definition_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                structure['has_definitions'] = True
                break

        # –ü–æ–∏—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤
        example_patterns = [
            r'–ü—Ä–∏–º–µ—Ä\s*[0-9]*[:.]',
            r'–†–∞—Å—Å–º–æ—Ç—Ä–∏–º\s+–ø—Ä–∏–º–µ—Ä',
            r'–í\s+–∫–∞—á–µ—Å—Ç–≤–µ\s+–ø—Ä–∏–º–µ—Ä–∞'
        ]
        for pattern in example_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                structure['has_examples'] = True
                break

        # –ü–æ–∏—Å–∫ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π
        exercise_patterns = [
            r'–ó–∞–¥–∞—á–∞\s*[0-9]*[:.]',
            r'–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ\s*[0-9]*[:.]',
            r'–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π\s+–≤–æ–ø—Ä–æ—Å',
            r'–°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–∞—è\s+—Ä–∞–±–æ—Ç–∞'
        ]
        for pattern in exercise_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                structure['has_exercises'] = True
                break

        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫
        reference_patterns = [
            r'\[[0-9]+\]',
            r'\([–ê-–Ø–∞-—è]+\s*,\s*\d{4}\)',
            r'–°–ø–∏—Å–æ–∫\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã',
            r'–ë–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è'
        ]
        for pattern in reference_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                structure['has_references'] = True
                break

        # –ü–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü –∏ —Ä–∏—Å—É–Ω–∫–æ–≤
        table_figure_patterns = [
            r'–¢–∞–±–ª–∏—Ü–∞\s*[0-9]*',
            r'–†–∏—Å\.\s*[0-9]*',
            r'–°—Ö–µ–º–∞\s*[0-9]*',
            r'–ì—Ä–∞—Ñ–∏–∫\s*[0-9]*'
        ]
        for pattern in table_figure_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                structure['has_tables_figures'] = True
                break

        return structure

    def _analyze_mathematical_content(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è"""
        math_analysis = {
            'has_formulas': False,
            'has_equations': False,
            'has_proofs': False,
            'has_theorems': False,
            'formula_density': 0,
            'math_keyword_count': 0
        }

        # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        math_keywords = [
            '—É—Ä–∞–≤–Ω–µ–Ω–∏–µ', '—Ñ–æ—Ä–º—É–ª–∞', '—Ç–µ–æ—Ä–µ–º–∞', '–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '—Ä–µ—à–µ–Ω–∏–µ',
            '–≤—ã—á–∏—Å–ª–∏—Ç—å', '—Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å', '—Ñ—É–Ω–∫—Ü–∏—è', '–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è', '–∏–Ω—Ç–µ–≥—Ä–∞–ª',
            '–º–∞—Ç—Ä–∏—Ü–∞', '–≤–µ–∫—Ç–æ—Ä', '–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', '–∞–ª–≥–æ—Ä–∏—Ç–º'
        ]

        # –ü–æ–¥—Å—á–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        text_lower = text.lower()
        math_analysis['math_keyword_count'] = sum(
            1 for keyword in math_keywords if keyword in text_lower
        )

        # –ü–æ–∏—Å–∫ —Ñ–æ—Ä–º—É–ª –∏ —É—Ä–∞–≤–Ω–µ–Ω–∏–π
        formula_patterns = [
            r'\$[^$]+\$',  # LaTeX
            r'\\[(\[]?[^\\]*?\\[\])]?',  # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
            r'[A-Za-z–ê-–Ø–∞-—èŒ±-œâŒë-Œ©]+\s*=\s*[^=\n]{3,}',  # –†–∞–≤–µ–Ω—Å—Ç–≤–∞ —Å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º
            r'\b\w+\s*[+\-*/^=<>‚â§‚â•‚â†]\s*\w+\b',  # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
        ]

        formula_count = 0
        for pattern in formula_patterns:
            matches = re.findall(pattern, text)
            formula_count += len(matches)
            if matches:
                math_analysis['has_formulas'] = True
                if '=' in pattern or '‚â†' in pattern or '‚â§' in pattern or '‚â•' in pattern:
                    math_analysis['has_equations'] = True

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—É–ª
        if len(text) > 0:
            math_analysis['formula_density'] = formula_count / (len(text) / 1000)

        # –ü–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –∏ —Ç–µ–æ—Ä–µ–º
        proof_theorem_patterns = [
            r'–¢–µ–æ—Ä–µ–º–∞\s*[0-9]*[:.]',
            r'–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\.',
            r'–õ–µ–º–º–∞\s*[0-9]*[:.]',
            r'–°–ª–µ–¥—Å—Ç–≤–∏–µ\s*[0-9]*[:.]',
            r'–¥–æ–∫–∞–∂–µ–º\b', r'–¥–æ–∫–∞–∑–∞—Ç—å\b'
        ]

        for pattern in proof_theorem_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                if '—Ç–µ–æ—Ä–µ–º–∞' in pattern.lower() or '–ª–µ–º–º–∞' in pattern.lower() or '—Å–ª–µ–¥—Å—Ç–≤–∏–µ' in pattern.lower():
                    math_analysis['has_theorems'] = True
                if '–¥–æ–∫–∞–∑–∞' in pattern.lower():
                    math_analysis['has_proofs'] = True

        return math_analysis

    def check_if_educational(self, text: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π"""
        # –î–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
        if len(text) > 2000:
            analysis_text = text[:1500]
        else:
            analysis_text = text

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = self._extract_text_features(analysis_text)
        structure = self._analyze_text_structure(analysis_text)
        math_content = self._analyze_mathematical_content(analysis_text)

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–∏–π —Å–∫–æ—Ä–∏–Ω–≥
        structural_score = (
            (1.0 if structure['has_numerical_sections'] else 0) * 0.25 +
            (1.0 if structure['has_definitions'] else 0) * 0.20 +
            (1.0 if structure['has_examples'] else 0) * 0.15 +
            (1.0 if structure['has_exercises'] else 0) * 0.20 +
            (1.0 if structure['has_references'] else 0) * 0.10 +
            (1.0 if structure['has_tables_figures'] else 0) * 0.10
        )

        mathematical_score = (
            (1.0 if math_content['has_formulas'] else 0) * 0.30 +
            (1.0 if math_content['has_equations'] else 0) * 0.25 +
            (1.0 if math_content['has_proofs'] else 0) * 0.15 +
            (1.0 if math_content['has_theorems'] else 0) * 0.15 +
            min(1.0, math_content['formula_density'] / 5.0) * 0.15
        )

        formal_score = min(1.0, features['vocabulary_richness'] * 1.5)

        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∏—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        total_score = (
            structural_score * 0.40 +
            mathematical_score * 0.35 +
            formal_score * 0.25
        )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π
        is_educational = total_score >= 0.5

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏
        details = []
        if structure['has_numerical_sections']:
            details.append("–∏–º–µ–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã")
        if structure['has_definitions']:
            details.append("—Å–æ–¥–µ—Ä–∂–∏—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤")
        if structure['has_exercises']:
            details.append("–≤–∫–ª—é—á–∞–µ—Ç —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∏ –∑–∞–¥–∞—á–∏")
        if math_content['has_formulas']:
            details.append("—Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã")

        if len(details) == 0:
            details.append("–Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —É—á–µ–±–Ω–∏–∫–∞")

        return {
            'is_educational': is_educational,
            'confidence': round(total_score, 2),
            'total_score': round(total_score, 2),
            'structural_score': round(structural_score, 2),
            'mathematical_score': round(mathematical_score, 2),
            'formal_score': round(formal_score, 2),
            'details': details[:3],
            'recommendation': '‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏' if is_educational else '‚ùå –ù–µ —è–≤–ª—è–µ—Ç—Å—è —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π'
        }


class PDFProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF —Ñ–∞–π–ª–æ–≤"""

    @staticmethod
    def clean_text(text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤"""
        return re.sub(r'\s+', ' ', text.strip())

    @staticmethod
    def extract_text(pdf_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–∞"""
        try:
            doc = fitz.open(pdf_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return PDFProcessor.clean_text(text)
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF: {str(e)}")

    @staticmethod
    def validate_text(text: str, min_length: int = 200) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        return text and len(text.strip()) >= min_length


class EmbeddingModel:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""

    def __init__(self, model_name: str = "Qwen/Qwen3-Embedding-0.6B"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()

        # –î–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–µ–≥–∞–º
        self.search_model = SentenceTransformer(
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )

    def get_text_embedding(self, text: str, chunk_size: int = 512) -> torch.Tensor:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        embeddings = []

        for chunk in chunks:
            inputs = self.tokenizer(
                chunk,
                return_tensors='pt',
                truncation=True,
                padding=True,
                max_length=512
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)

            emb = outputs.last_hidden_state.mean(dim=1)
            embeddings.append(emb.cpu())

        return torch.mean(torch.stack(embeddings), dim=0)

    def get_tag_embeddings(self, tags: List[str]) -> torch.Tensor:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–≥–æ–≤"""
        tag_embeddings = []

        for tag in tags:
            inputs = self.tokenizer(
                tag,
                return_tensors='pt',
                truncation=True,
                padding=True,
                max_length=10
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)

            tag_embedding = outputs.last_hidden_state.mean(dim=1)
            tag_embeddings.append(tag_embedding.cpu())

        return torch.cat(tag_embeddings)

    def find_similar_tags(self, query: str, all_tags: List[str], top_k: int = 8) -> List[str]:
        """–ü–æ–∏—Å–∫ —Ç–µ–≥–æ–≤, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ –∑–∞–ø—Ä–æ—Å"""
        query = query.lower()
        query = re.sub(r'[^\w\s–∞-—è—ë-]', ' ', query)
        query = re.sub(r'\s+', ' ', query).strip()

        if len(query) < 3:
            return []

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.search_model.encode(
            query,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö —Ç–µ–≥–æ–≤
        tag_embeddings = []
        for tag in all_tags:
            tag_emb = self.search_model.encode(
                tag,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            tag_embeddings.append(tag_emb)

        tag_embeddings = np.array(tag_embeddings)

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        similarities = cosine_similarity(
            query_embedding.reshape(1, -1),
            tag_embeddings
        )[0]

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
        sorted_indices = np.argsort(similarities)[::-1][:top_k]
        return [all_tags[i] for i in sorted_indices if similarities[i] > 0.3]


class TagManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–≥–∞–º–∏"""

    def __init__(self, tags_directory: str = "tags"):
        self.tags_directory = tags_directory
        self.tags_dict = self.load_tags()
        self.section_to_area = self.load_area_mapping()

    def load_tags(self) -> Dict[str, List[str]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        tags_dict = {}

        if not os.path.exists(self.tags_directory):
            os.makedirs(self.tags_directory)
            print(f"–°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {self.tags_directory}")
            return tags_dict

        for filename in os.listdir(self.tags_directory):
            if filename.endswith(".txt") and filename != "–æ–±–ª–∞—Å—Ç–∏_–∑–Ω–∞–Ω–∏–π.txt":
                category = filename[:-4]  # —É–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ .txt
                filepath = os.path.join(self.tags_directory, filename)

                with open(filepath, 'r', encoding='utf-8') as f:
                    tags = [line.strip() for line in f if line.strip()]
                    tags_dict[category] = tags
                    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(tags)} —Ç–µ–≥–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}'")

        return tags_dict

    def load_area_mapping(self, mapping_file: str = "tags/–æ–±–ª–∞—Å—Ç–∏_–∑–Ω–∞–Ω–∏–π.txt") -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π —Ä–∞–∑–¥–µ–ª–æ–≤ –∏ –æ–±–ª–∞—Å—Ç–µ–π –∑–Ω–∞–Ω–∏–π"""
        section_to_area = {}

        if os.path.exists(mapping_file):
            with open(mapping_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and ':' in line:
                        section, area = line.split(':', 1)
                        section_to_area[section.strip()] = area.strip()
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(section_to_area)} —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π —Ä–∞–∑–¥–µ–ª–æ–≤ –∏ –æ–±–ª–∞—Å—Ç–µ–π")
        else:
            print(f"–§–∞–π–ª —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è–º–∏ {mapping_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")

        return section_to_area

    def get_all_tags_flat(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–µ–≥–æ–≤ –≤ –æ–¥–Ω–æ–º —Å–ø–∏—Å–∫–µ"""
        all_tags = []
        for tags_list in self.tags_dict.values():
            all_tags.extend(tags_list)
        return all_tags

    def infer_area_from_sections(self, section_tags_found: List[str]) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–∏ –∑–Ω–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤"""
        area_counter = {}
        for section in section_tags_found:
            area = self.section_to_area.get(section)
            if area:
                area_counter[area] = area_counter.get(area, 0) + 1

        if area_counter:
            sorted_areas = sorted(area_counter.items(), key=lambda x: -x[1])
            return [sorted_areas[0][0]]
        return ["–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"]


class BookAnalyzer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–Ω–∏–≥"""

    def __init__(self, excel_file: str = "analyzed_books.xlsx"):
        self.excel_file = excel_file
        self.tag_manager = TagManager()
        self.embedding_model = EmbeddingModel()
        self.pdf_processor = PDFProcessor()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã
        self.educational_checker = AutonomousEducationalClassifier()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –∞–≥–µ–Ω—Ç–∞
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        self.ai_agent = EducationalAIAgent()
        print("AI –∞–≥–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")

    def get_next_book_number(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –Ω–æ–º–µ—Ä–∞ –∫–Ω–∏–≥–∏"""
        if not os.path.exists(self.excel_file):
            return 1

        try:
            df = pd.read_excel(self.excel_file)
            if df.empty or '–ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏' not in df.columns:
                return 1
            return df['–ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏'].max() + 1
        except:
            return 1

    def find_tags_for_text(self, text_embedding: torch.Tensor,
                          tag_list: List[str], top_k: int = 3) -> List[str]:
        """–ü–æ–∏—Å–∫ —Ç–µ–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if not tag_list:
            return []

        tag_embeddings = self.embedding_model.get_tag_embeddings(tag_list)
        similarities = F.cosine_similarity(text_embedding, tag_embeddings)
        top_indices = similarities.topk(min(top_k, len(tag_list))).indices
        return [tag_list[i] for i in top_indices]

    def analyze_book(self, pdf_path: str) -> Optional[BookData]:
        """–ê–Ω–∞–ª–∏–∑ –∫–Ω–∏–≥–∏ –∏–∑ PDF —Ñ–∞–π–ª–∞"""
        print(f"–ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {pdf_path}")

        if not os.path.exists(pdf_path):
            print(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")
            return None

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        print("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF...")
        raw_text = self.pdf_processor.extract_text(pdf_path)

        if not self.pdf_processor.validate_text(raw_text):
            print("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
            return None

        # üîç –ê–í–¢–û–ù–û–ú–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ù–ê –£–ß–ï–ë–ù–£–Æ –õ–ò–¢–ï–†–ê–¢–£–†–£
        print("üîç –ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—á–µ–±–Ω—É—é –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—É...")
        educational_check = self.educational_checker.check_if_educational(raw_text)

        print(f"   –û—Ü–µ–Ω–∫–∞ —É—á–µ–±–Ω–æ—Å—Ç–∏: {educational_check['total_score']:.2f}")
        print(f"   –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π —Å–∫–æ—Ä: {educational_check['structural_score']:.2f}")
        print(f"   –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–æ—Ä: {educational_check['mathematical_score']:.2f}")
        print(f"   –§–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å: {educational_check['formal_score']:.2f}")

        if not educational_check['is_educational']:
            print(f"‚ùå –§–∞–π–ª –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π!")
            print(f"   –ü—Ä–∏—á–∏–Ω–∞: {educational_check['recommendation']}")
            print(f"   –î–µ—Ç–∞–ª–∏: {', '.join(educational_check['details'])}")
            return None

        print(f"‚úÖ –ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {educational_check['recommendation']}")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç—É...")
        text_embedding = self.embedding_model.get_text_embedding(raw_text)

        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        print("–ê–Ω–∞–ª–∏–∑ —Ç–µ–≥–æ–≤...")
        found_tags = {}

        for category, tags in self.tag_manager.tags_dict.items():
            if category == "–æ–±–ª–∞—Å—Ç–∏_–∑–Ω–∞–Ω–∏–π":
                continue

            found_tags[category] = self.find_tags_for_text(text_embedding, tags, top_k=3)
            print(f"{category}: {', '.join(found_tags[category])}")

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–∏ –∑–Ω–∞–Ω–∏–π
        found_area = self.tag_manager.infer_area_from_sections(
            found_tags.get("—Ä–∞–∑–¥–µ–ª—ã", [])
        )

        # AI –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∫–Ω–∏–≥–∏
        print("–ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ AI –∞–Ω–∞–ª–∏–∑–∞...")
        ai_analysis = None
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            analysis_text = raw_text[:10000]  # –ü–µ—Ä–≤—ã–µ 10000 —Å–∏–º–≤–æ–ª–æ–≤
            ai_analysis = self.ai_agent.analyze_book_content(analysis_text, found_tags)
            print("AI –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")

            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã AI –∞–Ω–∞–ª–∏–∑–∞
            print("\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ AI –ê–ù–ê–õ–ò–ó–ê ===")
            print(f"–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ: {ai_analysis.summary[:200]}...")
            print(f"–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {ai_analysis.difficulty_level}")
            print(f"–û–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏: {', '.join(ai_analysis.mathematical_areas)}")
            print(f"–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã: {', '.join(ai_analysis.key_topics[:5])}")
            if ai_analysis.recommendations:
                print(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {ai_analysis.recommendations[0]}")
            print("=" * 40)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ AI –∞–Ω–∞–ª–∏–∑–µ: {str(e)}")
            print("–ü—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")

        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥–∏
        book_number = self.get_next_book_number()
        book_id = f"{book_number:04d}"

        return BookData(
            book_number=book_number,
            book_id=book_id,
            filename=os.path.basename(pdf_path),
            area=', '.join(found_area),
            tags=found_tags,
            text=raw_text[:500] + "..." if len(raw_text) > 500 else raw_text,
            embedding=text_embedding,
            ai_analysis=ai_analysis
        )

    def save_to_database(self, book_data: BookData):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥–∏ –≤ Excel"""
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        book_dict = {
            "–ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏": book_data.book_number,
            "ID –∫–Ω–∏–≥–∏": book_data.book_id,
            "–ò–º—è —Ñ–∞–π–ª–∞": book_data.filename,
            "–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π": book_data.area,
            "–¢–µ–∫—Å—Ç (—Ñ—Ä–∞–≥–º–µ–Ω—Ç)": book_data.text
        }

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category, tags in book_data.tags.items():
            book_dict[category.capitalize()] = ', '.join(tags)

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ AI –∞–Ω–∞–ª–∏–∑–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if book_data.ai_analysis:
            ai_data = book_data.ai_analysis

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º BookAnalysis –≤ —Å–ª–æ–≤–∞—Ä—å
            ai_dict = asdict(ai_data)

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            book_dict["AI_–†–µ–∑—é–º–µ"] = ai_dict['summary'][:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

            if ai_dict['sections_summary']:
                sections_str = ' | '.join(
                    f"{k}: {v[:100]}"
                    for k, v in list(ai_dict['sections_summary'].items())[:3]
                )
                book_dict["AI_–†–∞–∑–¥–µ–ª—ã"] = sections_str[:300]

            book_dict["AI_–ö–ª—é—á–µ–≤—ã–µ_—Ç–µ–º—ã"] = ', '.join(ai_dict['key_topics'][:5])
            book_dict["AI_–£—Ä–æ–≤–µ–Ω—å_—Å–ª–æ–∂–Ω–æ—Å—Ç–∏"] = ai_dict['difficulty_level']
            book_dict["AI_–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ_–∑–Ω–∞–Ω–∏—è"] = ', '.join(ai_dict['prerequisites'][:3])
            book_dict["AI_–û–±–ª–∞—Å—Ç–∏_–º–∞—Ç–µ–º–∞—Ç–∏–∫–∏"] = ', '.join(ai_dict['mathematical_areas'])
            book_dict["AI_–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"] = ' | '.join(ai_dict['recommendations'][:3])

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Excel
        df = pd.DataFrame([book_dict])

        if os.path.exists(self.excel_file):
            existing_df = pd.read_excel(self.excel_file)
            df = pd.concat([existing_df, df], ignore_index=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
        df.to_excel(self.excel_file, index=False)
        print(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {self.excel_file}")

        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
        if book_data.ai_analysis:
            self._save_ai_report(book_data)

    def _save_ai_report(self, book_data: BookData):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ AI –∞–Ω–∞–ª–∏–∑–∞"""
        report_dir = "ai_reports"
        if not os.path.exists(report_dir):
            os.makedirs(report_dir)

        report_file = os.path.join(report_dir, f"{book_data.book_id}_report.txt")

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"=== –û–¢–ß–ï–¢ AI –ê–ù–ê–õ–ò–ó–ê –ö–ù–ò–ì–ò ===\n\n")
            f.write(f"ID –∫–Ω–∏–≥–∏: {book_data.book_id}\n")
            f.write(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {book_data.filename}\n")
            f.write(f"–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π: {book_data.area}\n\n")

            if book_data.ai_analysis:
                ai = book_data.ai_analysis

                f.write("–ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï:\n")
                f.write(f"{ai.summary}\n\n")

                f.write("–£–†–û–í–ï–ù–¨ –°–õ–û–ñ–ù–û–°–¢–ò:\n")
                f.write(f"{ai.difficulty_level}\n\n")

                if ai.mathematical_areas:
                    f.write("–û–ë–õ–ê–°–¢–ò –ú–ê–¢–ï–ú–ê–¢–ò–ö–ò:\n")
                    for area in ai.mathematical_areas:
                        f.write(f"- {area}\n")
                    f.write("\n")

                if ai.key_topics:
                    f.write("–ö–õ–Æ–ß–ï–í–´–ï –¢–ï–ú–´:\n")
                    for topic in ai.key_topics[:10]:
                        f.write(f"- {topic}\n")
                    f.write("\n")

                if ai.prerequisites:
                    f.write("–ù–ï–û–ë–•–û–î–ò–ú–´–ï –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ï –ó–ù–ê–ù–ò–Ø:\n")
                    for prereq in ai.prerequisites:
                        f.write(f"- {prereq}\n")
                    f.write("\n")

                if ai.sections_summary:
                    f.write("–ê–ù–ê–õ–ò–ó –†–ê–ó–î–ï–õ–û–í:\n")
                    for section, desc in list(ai.sections_summary.items())[:5]:
                        f.write(f"{section}:\n")
                        f.write(f"{desc[:200]}...\n\n")

                if ai.recommendations:
                    f.write("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–ó–£–ß–ï–ù–ò–Æ:\n")
                    for rec in ai.recommendations:
                        f.write(f"- {rec}\n")

        print(f"–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")

    def search_books(self, query: str, top_k: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –∫–Ω–∏–≥ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not os.path.exists(self.excel_file):
            return []

        try:
            df = pd.read_excel(self.excel_file)

            # –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–≥–∞–º
            all_tags = self.tag_manager.get_all_tags_flat()
            similar_tags = self.embedding_model.find_similar_tags(query, all_tags)

            if not similar_tags:
                return []

            # –ò—â–µ–º –∫–Ω–∏–≥–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–≥–∏
            results = []
            for _, row in df.iterrows():
                score = 0
                book_tags = []

                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–≥–∏ –∫–Ω–∏–≥–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                for col in df.columns:
                    if col not in ["–ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏", "ID –∫–Ω–∏–≥–∏", "–ò–º—è —Ñ–∞–π–ª–∞",
                                 "–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π", "–¢–µ–∫—Å—Ç (—Ñ—Ä–∞–≥–º–µ–Ω—Ç)"] and not col.startswith("AI_"):
                        if pd.notna(row[col]):
                            book_tags.extend(str(row[col]).split(', '))

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                for tag in similar_tags:
                    if tag in book_tags:
                        score += 1

                if score > 0:
                    result = {
                        'score': score,
                        'book_id': row['ID –∫–Ω–∏–≥–∏'],
                        'filename': row['–ò–º—è —Ñ–∞–π–ª–∞'],
                        'area': row['–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π'],
                        'matching_tags': [t for t in similar_tags if t in book_tags]
                    }

                    # –î–æ–±–∞–≤–ª—è–µ–º AI –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                    if 'AI_–†–µ–∑—é–º–µ' in df.columns and pd.notna(row['AI_–†–µ–∑—é–º–µ']):
                        result['ai_summary'] = str(row['AI_–†–µ–∑—é–º–µ'])[:150] + "..."

                    if 'AI_–£—Ä–æ–≤–µ–Ω—å_—Å–ª–æ–∂–Ω–æ—Å—Ç–∏' in df.columns and pd.notna(row['AI_–£—Ä–æ–≤–µ–Ω—å_—Å–ª–æ–∂–Ω–æ—Å—Ç–∏']):
                        result['ai_difficulty'] = str(row['AI_–£—Ä–æ–≤–µ–Ω—å_—Å–ª–æ–∂–Ω–æ—Å—Ç–∏'])

                    results.append(result)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            results.sort(key=lambda x: x['score'], reverse=True)
            return results[:top_k]

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {str(e)}")
            return []

    def get_book_details(self, book_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ"""
        if not os.path.exists(self.excel_file):
            return None

        try:
            df = pd.read_excel(self.excel_file)
            book_row = df[df['ID –∫–Ω–∏–≥–∏'] == book_id]

            if book_row.empty:
                return None

            row = book_row.iloc[0]

            # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            details = {
                'book_id': row['ID –∫–Ω–∏–≥–∏'],
                'filename': row['–ò–º—è —Ñ–∞–π–ª–∞'],
                'area': row['–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π'],
                'tags': {}
            }

            # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–≥–∏
            for col in df.columns:
                if col not in ["–ù–æ–º–µ—Ä –∫–Ω–∏–≥–∏", "ID –∫–Ω–∏–≥–∏", "–ò–º—è —Ñ–∞–π–ª–∞",
                             "–û–±–ª–∞—Å—Ç—å –∑–Ω–∞–Ω–∏–π", "–¢–µ–∫—Å—Ç (—Ñ—Ä–∞–≥–º–µ–Ω—Ç)"] and not col.startswith("AI_"):
                    if pd.notna(row[col]):
                        details['tags'][col] = str(row[col]).split(', ')

            # –î–æ–±–∞–≤–ª—è–µ–º AI –¥–∞–Ω–Ω—ã–µ
            ai_fields = [col for col in df.columns if col.startswith("AI_")]
            if ai_fields:
                details['ai_analysis'] = {}
                for field in ai_fields:
                    if pd.notna(row[field]):
                        details['ai_analysis'][field.replace("AI_", "")] = str(row[field])

            return details

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–µ—Ç–∞–ª–µ–π –∫–Ω–∏–≥–∏: {str(e)}")
            return None