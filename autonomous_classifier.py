"""
AI –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã
"""
import re
import torch
from typing import Dict, List, Optional, Tuple, Any
from transformers import AutoTokenizer, AutoModel, pipeline
import numpy as np


class AutonomousEducationalClassifier:
    """–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Qwen"""

    def __init__(self, model_name: str = "Qwen/Qwen3-Embedding-0.6B"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device} –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —É—á–µ–±–Ω–æ—Å—Ç–∏")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name).to(self.device)
            self.model.eval()

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–±–æ–ª–µ–µ –ª–µ–≥–∫—É—é –≤–µ—Ä—Å–∏—é)
            try:
                # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                self.text_model_name = "cointegrated/rubert-tiny2"
                self.text_tokenizer = AutoTokenizer.from_pretrained(self.text_model_name)
                self.text_model = AutoModel.from_pretrained(self.text_model_name).to(self.device)
                self.text_model.eval()
            except:
                print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—é –æ—Å–Ω–æ–≤–Ω—É—é")
                self.text_tokenizer = self.tokenizer
                self.text_model = self.model

            print("‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            self.tokenizer = None
            self.model = None
            self.text_tokenizer = None
            self.text_model = None

    def _extract_text_features(self, text: str) -> Dict[str, Any]:
        """–ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        features = {
            'text_length': len(text),
            'paragraph_count': len(re.split(r'\n\s*\n', text)),
            'sentence_count': len(re.split(r'[.!?]+', text)),
            'avg_sentence_length': 0,
            'vocabulary_richness': 0,
            'formality_score': 0,
            'structure_score': 0
        }

        if features['sentence_count'] > 0:
            features['avg_sentence_length'] = len(text) / features['sentence_count']

        # –ê–Ω–∞–ª–∏–∑ –±–æ–≥–∞—Ç—Å—Ç–≤–∞ —Å–ª–æ–≤–∞—Ä–Ω–æ–≥–æ –∑–∞–ø–∞—Å–∞
        words = re.findall(r'\b[–∞-—è–ê-–Ø—ë–Å]{3,}\b', text.lower())
        if words:
            unique_words = set(words)
            features['vocabulary_richness'] = len(unique_words) / len(words) if len(words) > 0 else 0

        return features

    def _analyze_text_structure(self, text: str) -> Dict[str, Any]:
        """–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–µ–∫—Å—Ç–∞"""
        structure = {
            'has_numerical_sections': False,
            'has_definitions': False,
            'has_examples': False,
            'has_exercises': False,
            'has_references': False,
            'has_tables_figures': False,
            'section_hierarchy_depth': 0
        }

        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        headings = re.findall(r'(?:–ì–ª–∞–≤–∞|–†–∞–∑–¥–µ–ª|¬ß|–¢–µ–º–∞|–ü–∞—Ä–∞–≥—Ä–∞—Ñ|–ß–∞—Å—Ç—å)\s+[^\n]+', text)
        if headings:
            structure['has_numerical_sections'] = True
            structure['section_hierarchy_depth'] = min(3, len(headings) // 2)

        # –ü–æ–∏—Å–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
        definition_patterns = [
            r'–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\s*[0-9]*[:.]?\s*[^\n]+',
            r'\b–æ–ø—Ä–µ–¥–µ–ª–∏–º\b.*?–∫–∞–∫\b',
            r'\b–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è\b.*?\b–µ—Å–ª–∏\b'
        ]
        for pattern in definition_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                structure['has_definitions'] = True
                break

        # –ü–æ–∏—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤
        example_patterns = [
            r'–ü—Ä–∏–º–µ—Ä\s*[0-9]*[:.]',
            r'–†–∞—Å—Å–º–æ—Ç—Ä–∏–º\s+–ø—Ä–∏–º–µ—Ä',
            r'–í\s+–∫–∞—á–µ—Å—Ç–≤–µ\s+–ø—Ä–∏–º–µ—Ä–∞'
        ]
        for pattern in example_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                structure['has_examples'] = True
                break

        # –ü–æ–∏—Å–∫ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π
        exercise_patterns = [
            r'–ó–∞–¥–∞—á–∞\s*[0-9]*[:.]',
            r'–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ\s*[0-9]*[:.]',
            r'–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π\s+–≤–æ–ø—Ä–æ—Å',
            r'–°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–∞—è\s+—Ä–∞–±–æ—Ç–∞'
        ]
        for pattern in exercise_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                structure['has_exercises'] = True
                break

        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫
        reference_patterns = [
            r'\[[0-9]+\]',
            r'\([–ê-–Ø–∞-—è]+\s*,\s*\d{4}\)',
            r'–°–ø–∏—Å–æ–∫\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã',
            r'–ë–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—è'
        ]
        for pattern in reference_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                structure['has_references'] = True
                break

        # –ü–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü –∏ —Ä–∏—Å—É–Ω–∫–æ–≤
        table_figure_patterns = [
            r'–¢–∞–±–ª–∏—Ü–∞\s*[0-9]*',
            r'–†–∏—Å\.\s*[0-9]*',
            r'–°—Ö–µ–º–∞\s*[0-9]*',
            r'–ì—Ä–∞—Ñ–∏–∫\s*[0-9]*'
        ]
        for pattern in table_figure_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                structure['has_tables_figures'] = True
                break

        return structure

    def _analyze_mathematical_content(self, text: str) -> Dict[str, Any]:
        """–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è"""
        math_analysis = {
            'has_formulas': False,
            'has_equations': False,
            'has_proofs': False,
            'has_theorems': False,
            'formula_density': 0,
            'math_keyword_count': 0
        }

        # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏)
        math_keywords = [
            '—É—Ä–∞–≤–Ω–µ–Ω–∏–µ', '—Ñ–æ—Ä–º—É–ª–∞', '—Ç–µ–æ—Ä–µ–º–∞', '–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '—Ä–µ—à–µ–Ω–∏–µ',
            '–≤—ã—á–∏—Å–ª–∏—Ç—å', '—Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å', '—Ñ—É–Ω–∫—Ü–∏—è', '–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è', '–∏–Ω—Ç–µ–≥—Ä–∞–ª',
            '–º–∞—Ç—Ä–∏—Ü–∞', '–≤–µ–∫—Ç–æ—Ä', '–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', '–∞–ª–≥–æ—Ä–∏—Ç–º'
        ]

        # –ü–æ–¥—Å—á–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        text_lower = text.lower()
        math_analysis['math_keyword_count'] = sum(
            1 for keyword in math_keywords if keyword in text_lower
        )

        # –ü–æ–∏—Å–∫ —Ñ–æ—Ä–º—É–ª –∏ —É—Ä–∞–≤–Ω–µ–Ω–∏–π
        formula_patterns = [
            r'\$[^$]+\$',  # LaTeX
            r'\\[(\[]?[^\\]*?\\[\])]?',  # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
            r'[A-Za-z–ê-–Ø–∞-—èŒ±-œâŒë-Œ©]+\s*=\s*[^=\n]{3,}',  # –†–∞–≤–µ–Ω—Å—Ç–≤–∞ —Å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º
            r'\b\w+\s*[+\-*/^=<>‚â§‚â•‚â†]\s*\w+\b',  # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
        ]

        formula_count = 0
        for pattern in formula_patterns:
            matches = re.findall(pattern, text)
            formula_count += len(matches)
            if matches:
                math_analysis['has_formulas'] = True
                if '=' in pattern or '‚â†' in pattern or '‚â§' in pattern or '‚â•' in pattern:
                    math_analysis['has_equations'] = True

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—É–ª
        if len(text) > 0:
            math_analysis['formula_density'] = formula_count / (len(text) / 1000)

        # –ü–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –∏ —Ç–µ–æ—Ä–µ–º
        proof_theorem_patterns = [
            r'–¢–µ–æ—Ä–µ–º–∞\s*[0-9]*[:.]',
            r'–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ\.',
            r'–õ–µ–º–º–∞\s*[0-9]*[:.]',
            r'–°–ª–µ–¥—Å—Ç–≤–∏–µ\s*[0-9]*[:.]',
            r'–¥–æ–∫–∞–∂–µ–º\b', r'–¥–æ–∫–∞–∑–∞—Ç—å\b'
        ]

        for pattern in proof_theorem_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                if '—Ç–µ–æ—Ä–µ–º–∞' in pattern.lower() or '–ª–µ–º–º–∞' in pattern.lower() or '—Å–ª–µ–¥—Å—Ç–≤–∏–µ' in pattern.lower():
                    math_analysis['has_theorems'] = True
                if '–¥–æ–∫–∞–∑–∞' in pattern.lower():
                    math_analysis['has_proofs'] = True

        return math_analysis

    def _get_semantic_embedding(self, text: str, max_length: int = 512) -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞"""
        if self.model is None or self.tokenizer is None:
            return None

        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if len(text) > 2000:
                text = text[:2000]

            inputs = self.tokenizer(
                text,
                return_tensors='pt',
                truncation=True,
                padding=True,
                max_length=max_length
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–∏–π –ø—É–ª
                embeddings = outputs.last_hidden_state.mean(dim=1)
                return embeddings.cpu().numpy()[0]

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return None

    def _generate_self_learning_features(self, text: str) -> Dict[str, float]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ª–µ—Ç—É"""
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
        embedding = self._get_semantic_embedding(text)

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features = self._extract_text_features(text)
        structure = self._analyze_text_structure(text)
        math_content = self._analyze_mathematical_content(text)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        all_features = {}

        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–µ–∫—Å—Ç–∞
        all_features['text_length_norm'] = min(1.0, features['text_length'] / 5000)
        all_features['vocabulary_richness'] = features['vocabulary_richness']

        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        structure_features = [
            'has_numerical_sections',
            'has_definitions',
            'has_examples',
            'has_exercises',
            'has_references',
            'has_tables_figures'
        ]

        for feature in structure_features:
            all_features[feature] = 1.0 if structure[feature] else 0.0

        all_features['section_depth_norm'] = structure['section_hierarchy_depth'] / 3.0

        # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        math_features = [
            'has_formulas',
            'has_equations',
            'has_proofs',
            'has_theorems'
        ]

        for feature in math_features:
            all_features[feature] = 1.0 if math_content[feature] else 0.0

        all_features['math_keyword_density'] = min(1.0, math_content['math_keyword_count'] / 10.0)
        all_features['formula_density_norm'] = min(1.0, math_content['formula_density'] / 5.0)

        # –ï—Å–ª–∏ –µ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥, –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –µ–≥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if embedding is not None:
            all_features['embedding_norm'] = float(np.linalg.norm(embedding))
            all_features['embedding_mean'] = float(np.mean(embedding))
            all_features['embedding_std'] = float(np.std(embedding))

        return all_features

    def _make_autonomous_decision(self, features: Dict[str, float]) -> Dict[str, Any]:
        """–ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤
        weights = {
            'structural': 0.35,  # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —É—á–µ–±–Ω–∏–∫–∞
            'mathematical': 0.30,  # –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
            'formal': 0.20,  # –§–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å —è–∑—ã–∫–∞
            'compositional': 0.15  # –ö–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
        }

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Ü–µ–Ω–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—Å–ø–µ–∫—Ç–∞

        # 1. –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        structural_score = (
                                   features.get('has_numerical_sections', 0) * 0.3 +
                                   features.get('has_definitions', 0) * 0.2 +
                                   features.get('has_examples', 0) * 0.15 +
                                   features.get('has_exercises', 0) * 0.2 +
                                   features.get('has_references', 0) * 0.15
                           ) * weights['structural']

        # 2. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
        mathematical_score = (
                                     features.get('has_formulas', 0) * 0.25 +
                                     features.get('has_equations', 0) * 0.25 +
                                     features.get('has_proofs', 0) * 0.15 +
                                     features.get('has_theorems', 0) * 0.15 +
                                     features.get('formula_density_norm', 0) * 0.2
                             ) * weights['mathematical']

        # 3. –û—Ü–µ–Ω–∫–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏
        formal_score = (
                               features.get('vocabulary_richness', 0) * 0.4 +
                               features.get('section_depth_norm', 0) * 0.3 +
                               features.get('has_tables_figures', 0) * 0.3
                       ) * weights['formal']

        # 4. –ö–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        compositional_score = (
                                      features.get('text_length_norm', 0) * 0.6 +
                                      min(1.0, features.get('math_keyword_density', 0) * 2) * 0.4
                              ) * weights['compositional']

        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        total_score = structural_score + mathematical_score + formal_score + compositional_score

        # –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
        sub_scores = [structural_score, mathematical_score, formal_score, compositional_score]
        score_variance = np.var(sub_scores) if len(sub_scores) > 1 else 0

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–æ–∫
        if score_variance < 0.05:  # –û—Ü–µ–Ω–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã
            threshold = 0.45
        else:  # –û—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã
            threshold = 0.55

        # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ
        is_educational = total_score >= threshold

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = min(0.95, total_score * 1.2)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–≥–æ –∞—Å–ø–µ–∫—Ç–∞
        content_type = "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"
        if is_educational:
            if mathematical_score > structural_score and mathematical_score > formal_score:
                content_type = "—É—á–µ–±–Ω—ã–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π"
            elif structural_score > mathematical_score:
                content_type = "—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —É—á–µ–±–Ω—ã–π"
            else:
                content_type = "—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —É—á–µ–±–Ω—ã–π"
        else:
            if total_score < 0.3:
                content_type = "—Ä–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π/—Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π"
            elif mathematical_score < 0.1:
                content_type = "–Ω–µ–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π/–Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π"
            else:
                content_type = "—Å–º–µ—à–∞–Ω–Ω—ã–π/–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π"

        return {
            'is_educational': is_educational,
            'confidence': round(confidence, 3),
            'total_score': round(total_score, 3),
            'content_type': content_type,
            'sub_scores': {
                'structural': round(structural_score, 3),
                'mathematical': round(mathematical_score, 3),
                'formal': round(formal_score, 3),
                'compositional': round(compositional_score, 3)
            },
            'threshold_used': round(threshold, 3),
            'score_variance': round(score_variance, 3)
        }

    def analyze_autonomously(self, text: str, fast_check: bool = True) -> Dict[str, Any]:
        """
        –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —É—á–µ–±–Ω–æ—Å—Ç—å

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            fast_check: –ï—Å–ª–∏ True, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞

        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        if fast_check and len(text) > 1500:
            # –î–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞
            analysis_text = text[:1000] + text[-500:] if len(text) > 1500 else text[:1000]
        else:
            analysis_text = text

        print("üîç –ù–∞—á–∏–Ω–∞—é –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞...")

        # 1. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = self._generate_self_learning_features(analysis_text)

        # 2. –ü—Ä–∏–Ω–∏–º–∞–µ–º –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
        decision = self._make_autonomous_decision(features)

        # 3. –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
        report = {
            'decision': decision,
            'analysis_metadata': {
                'text_length_analyzed': len(analysis_text),
                'total_text_length': len(text),
                'fast_check_used': fast_check,
                'features_extracted': len(features)
            },
            'key_findings': self._extract_key_findings(analysis_text, features),
            'recommendation': self._generate_recommendation(decision, features)
        }

        return report

    def _extract_key_findings(self, text: str, features: Dict[str, float]) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞—Ö–æ–¥–æ–∫ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞"""
        findings = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        if features.get('has_numerical_sections', 0) > 0.5:
            findings.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —á–µ—Ç–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏")

        if features.get('has_definitions', 0) > 0.5:
            findings.append("–ù–∞–π–¥–µ–Ω—ã —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤")

        if features.get('has_exercises', 0) > 0.5:
            findings.append("–ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∏ –∑–∞–¥–∞—á–∏")

        if features.get('has_formulas', 0) > 0.5:
            findings.append("–°–æ–¥–µ—Ä–∂–∏—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã –∏ —É—Ä–∞–≤–Ω–µ–Ω–∏—è")

        if features.get('has_references', 0) > 0.5:
            findings.append("–ï—Å—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—É –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")

        # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if features.get('formula_density_norm', 0) > 0.7:
            findings.append("–í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π")

        if features.get('section_depth_norm', 0) > 0.7:
            findings.append("–ì–ª—É–±–æ–∫–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞")

        # –ï—Å–ª–∏ –Ω–∞—Ö–æ–¥–æ–∫ –º–∞–ª–æ, –¥–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ
        if len(findings) < 2:
            if len(text) > 1000:
                findings.append("–î–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            if features.get('vocabulary_richness', 0) > 0.6:
                findings.append("–ë–æ–≥–∞—Ç—ã–π –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å")

        return findings[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 5 –Ω–∞—Ö–æ–¥–∫–∞–º–∏

    def _generate_recommendation(self, decision: Dict[str, Any], features: Dict[str, float]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ—à–µ–Ω–∏—è"""
        if decision['is_educational']:
            confidence = decision['confidence']

            if confidence > 0.8:
                return "‚úÖ –í—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑."
            elif confidence > 0.6:
                return "‚úÖ –£–º–µ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π."
            else:
                return "‚ö†Ô∏è  –ù–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—á–µ–±–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞."
        else:
            if decision['total_score'] < 0.3:
                return "‚ùå –í–µ—Ä–æ—è—Ç–Ω–æ, –Ω–µ —É—á–µ–±–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—Ç–∫–ª–æ–Ω–∏—Ç—å."
            elif decision['total_score'] < 0.5:
                return "‚ö†Ô∏è  –°–æ–º–Ω–∏—Ç–µ–ª—å–Ω–∞—è —É—á–µ–±–Ω–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å. –¢—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞."
            else:
                return "‚ö†Ô∏è  –ü–æ–≥—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞."

    def batch_analyze(self, texts: List[str], max_workers: int = 2) -> List[Dict[str, Any]]:
        """–ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        results = []

        for i, text in enumerate(texts):
            print(f"–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ {i + 1}/{len(texts)}...")
            result = self.analyze_autonomously(text, fast_check=True)
            results.append(result)

        return results


# –£—Ç–∏–ª–∏—Ç–∞—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
def create_quick_classifier():
    """–°–æ–∑–¥–∞–Ω–∏–µ –±—ã—Å—Ç—Ä–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
    return AutonomousEducationalClassifier()


def check_if_educational(text: str, classifier: Optional[AutonomousEducationalClassifier] = None) -> Dict[str, Any]:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É—á–µ–±–Ω–æ—Å—Ç–∏

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        classifier: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä

    Returns:
        –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏
    """
    if classifier is None:
        classifier = AutonomousEducationalClassifier()

    result = classifier.analyze_autonomously(text, fast_check=True)

    # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
    simplified_result = {
        'is_educational': result['decision']['is_educational'],
        'confidence': result['decision']['confidence'],
        'content_type': result['decision']['content_type'],
        'recommendation': result['recommendation']
    }

    return simplified_result